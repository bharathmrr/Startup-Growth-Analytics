# Startup Growth Analytics - Project Summary

## ğŸ¯ Project Overview

A comprehensive MLOps pipeline for analyzing startup growth patterns and predicting success metrics using machine learning.

## ğŸ“Š Dataset

**File:** `data/raw/startup_data.csv`
- **Total Records:** 500+ startups
- **Features:** 12 columns
  - Startup Name, Industry, Funding Rounds
  - Funding Amount (M USD), Valuation (M USD), Revenue (M USD)
  - Employees, Market Share (%), Profitable
  - Year Founded, Region, Exit Status

## ğŸ”„ MLOps Pipeline Stages

### 1ï¸âƒ£ Data Ingestion
- **Module:** `src/data_ingestion.py`
- **Purpose:** Load and clean raw data
- **Actions:**
  - Handle missing values (median/mode imputation)
  - Remove duplicates
  - Basic data validation
- **Output:** `data/processed/processed_data.csv`

### 2ï¸âƒ£ Feature Engineering
- **Module:** `src/feature_engineering.py`
- **Purpose:** Create and transform features
- **Actions:**
  - Create `Startup_Age` = 2025 - Year Founded
  - Create `Success_Status` based on:
    - Funding > $100M USD
    - Employees > 1000
    - Valuation > $500M USD
  - Label encode categorical variables
  - Normalize numerical features (StandardScaler)
  - Split into train/test (80/20)
- **Outputs:**
  - `data/processed/train_data.csv`
  - `data/processed/test_data.csv`
  - `data/final/final_data.csv`
  - `models/label_encoders.joblib`

### 3ï¸âƒ£ Success Scoring
- **Module:** `src/success_scoring.py`
- **Purpose:** Analyze success patterns
- **Visualizations:**
  - Success distribution pie chart
  - Success rate by industry
  - Success rate by region
  - Funding vs success histogram
  - Employees vs success histogram
  - Correlation heatmap
- **Output:** `reports/success_analysis.json` + 6 PNG charts

### 4ï¸âƒ£ Model Training
- **Module:** `src/model_training.py`
- **Algorithm:** Random Forest Classifier
- **Hyperparameters:**
  - n_estimators: 100
  - max_depth: 10
  - min_samples_split: 5
  - min_samples_leaf: 2
  - class_weight: balanced
- **Outputs:**
  - `models/startup_success_model.joblib`
  - `reports/model_metrics.json`
  - Confusion matrix & feature importance plots

### 5ï¸âƒ£ Evaluation
- **Module:** `src/evaluation.py`
- **Metrics:**
  - Accuracy, Precision, Recall, F1 Score, ROC AUC
- **Visualizations:**
  - ROC curve
  - Precision-Recall curve
  - Detailed confusion matrix
  - Metrics comparison bar chart
- **Outputs:**
  - `reports/evaluation_metrics.json`
  - `reports/evaluation_report.txt`
  - 4 evaluation plots

### 6ï¸âƒ£ Deployment
- **Module:** `src/deployment.py`
- **Purpose:** Prepare model for production
- **Artifacts:**
  - Flask REST API (`deployment/api.py`)
  - Dockerfile for containerization
  - Requirements file
  - Deployment documentation
  - Model metadata

## ğŸ“ Project Structure

```
Startup-Growth-Analytics/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original data
â”‚   â”œâ”€â”€ processed/              # Cleaned & split data
â”‚   â””â”€â”€ final/                  # Final dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion.py       # Stage 1
â”‚   â”œâ”€â”€ feature_engineering.py  # Stage 2
â”‚   â”œâ”€â”€ success_scoring.py      # Stage 3
â”‚   â”œâ”€â”€ model_training.py       # Stage 4
â”‚   â”œâ”€â”€ evaluation.py           # Stage 5
â”‚   â””â”€â”€ deployment.py           # Stage 6
â”œâ”€â”€ models/                     # Trained models
â”œâ”€â”€ reports/                    # Analysis & metrics
â”‚   â””â”€â”€ figures/                # All visualizations
â”œâ”€â”€ deployment/                 # Production artifacts
â”œâ”€â”€ logs/                       # Execution logs
â”œâ”€â”€ config/
â”‚   â””â”€â”€ params.yaml            # Configuration
â”œâ”€â”€ dvc.yaml                   # DVC pipeline
â”œâ”€â”€ run_pipeline.py            # Main runner
â”œâ”€â”€ quick_start.bat            # Windows quick start
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ PIPELINE_GUIDE.md          # Detailed guide
â””â”€â”€ README.md                  # Project README
```

## ğŸš€ Quick Start

### Method 1: Windows Batch Script
```bash
quick_start.bat
```

### Method 2: Python Script
```bash
python run_pipeline.py
```

### Method 3: Individual Stages
```bash
python src/data_ingestion.py
python src/feature_engineering.py
python src/success_scoring.py
python src/model_training.py
python src/evaluation.py
python src/deployment.py
```

### Method 4: DVC Pipeline
```bash
dvc repro
```

## ğŸ“ˆ Expected Outputs

### Data Outputs
- âœ… Cleaned dataset (500+ records)
- âœ… Train/test split (400/100 approx.)
- âœ… Encoded & normalized features

### Model Outputs
- âœ… Trained Random Forest model
- âœ… Label encoders for categorical variables
- âœ… Model performance metrics

### Visualization Outputs (12+ charts)
- âœ… Success distribution analysis
- âœ… Industry & region insights
- âœ… Feature correlations
- âœ… Model performance curves
- âœ… Confusion matrices
- âœ… Feature importance

### Deployment Outputs
- âœ… REST API template
- âœ… Docker configuration
- âœ… Deployment documentation

## ğŸ“ Key Insights Expected

1. **Success Rate:** ~X% of startups meet success criteria
2. **Top Industries:** Which sectors have highest success rates
3. **Regional Patterns:** Geographic distribution of success
4. **Funding Impact:** Correlation between funding and success
5. **Model Performance:** Accuracy, precision, recall metrics
6. **Feature Importance:** Which factors most predict success

## ğŸ”§ Configuration

Edit `config/params.yaml` to customize:

```yaml
# Success definition thresholds
success_thresholds:
  funding_amount: 100   # M USD
  employees: 1000
  valuation: 500        # M USD

# Model parameters
test_size: 0.2
random_state: 42
```

## ğŸ“Š Logging

All stages log to:
- **Console:** Real-time progress
- **File:** `logs/startup_analytics.log`
- **Pipeline:** `logs/pipeline.log`

## ğŸ³ Deployment

### Local API Testing
```bash
cd deployment
python api.py
```

### Docker Deployment
```bash
cd deployment
docker build -t startup-predictor .
docker run -p 5000:5000 startup-predictor
```

### API Usage
```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "Funding Amount (M USD)": 150.0,
    "Valuation (M USD)": 1200.0,
    "Revenue (M USD)": 80.0,
    "Employees": 2500,
    "Market Share (%)": 7.5,
    "Year Founded": 2015,
    "Industry": "FinTech",
    "Region": "North America"
  }'
```

## ğŸ“š Documentation

- **README.md** - Project overview
- **PIPELINE_GUIDE.md** - Detailed pipeline documentation
- **PROJECT_SUMMARY.md** - This file
- **deployment/README.md** - Deployment guide
- **reports/evaluation_report.txt** - Model evaluation

## ğŸ¯ Success Criteria

âœ… Clean, reproducible pipeline
âœ… Comprehensive data analysis
âœ… Trained ML model with >70% accuracy
âœ… Detailed visualizations and insights
âœ… Production-ready deployment artifacts
âœ… Complete documentation

## ğŸ”„ Maintenance

### Retraining
1. Add new data to `data/raw/startup_data.csv`
2. Run: `python run_pipeline.py`
3. Compare metrics in `reports/`
4. Redeploy if improved

### Monitoring
- Track model performance over time
- Monitor prediction accuracy
- Update thresholds as needed
- Retrain quarterly or when drift detected

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Make changes
4. Run pipeline to test
5. Submit pull request

## ğŸ“„ License

MIT License - See LICENSE file

---

**Built with â¤ï¸ for Data Science & MLOps**
